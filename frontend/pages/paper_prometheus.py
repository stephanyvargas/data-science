import streamlit as st

def load_page():
    # **PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models**
    st.header("PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models")
    st.write("Summary based on the paper: [https://arxiv.org/pdf/2405.01535](https://arxiv.org/pdf/2405.01535)")
    st.subheader("Abstract")
    st.write("""
    Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including 
    transparency, controllability, and affordability strongly motivate the development of opensource LMs specialized in evaluations. On the
    other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned 
    by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment.
    Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like
    helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than itâ€™s predecessor that
    closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats
    grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, PROMETHEUS
    2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models,
    code, and data are all publicly available in [https://github.com/prometheus-eval/prometheus-eval](https://github.com/prometheus-eval/prometheus-eval).
    """)
    
    st.markdown("## Summary")
    
    st.markdown("Evaluating the quality of text generated by large language models (LLMs) is becoming increasingly challenging due to the vast diversity of outputs and complex tasks they can perform.")
    
    st.markdown("**Language Model-Based Evaluation**")
    
    st.markdown("- A scalable and cost-effective approach for assessing LM-generated text.")
    st.markdown("- LMs can be prompted to: ")
    st.markdown("    - Provide a quality score (direct assessment).")
    st.markdown("    - Determine which of two outputs is better (pairwise ranking).")
    
    st.markdown("**Challenges of Proprietary LMs for Evaluation**")
    
    st.markdown("- Lack of transparency in training data raises concerns about fairness, compliance, and ethical usage.")
    st.markdown("- Controllability and affordability can be limitations.")
    
    st.markdown("**Open-Source Evaluator LMs**")
    
    st.markdown("- More transparent and controllable alternative.")
    st.markdown("- Often struggle to match human or proprietary LM evaluation quality.")
    st.markdown("- Limited flexibility, typically trained for a single evaluation task (direct assessment or pairwise ranking).")
    
    st.markdown("**Introducing PROMETHEUS 2**")
    
    st.markdown("- A unified evaluator LM that excels in both direct assessment and pairwise ranking.")
    st.markdown("- Achieves high correlations with human evaluators and proprietary LM judges.")
    st.markdown("- Leverages a new dataset (PREFERENCE COLLECTION) for fine-grained pairwise ranking feedback.")
    
    st.markdown("**Key Contributions**")
    
    st.markdown("- State-of-the-art open evaluator LM: PROMETHEUS 2.")
    st.markdown("- PREFERENCE COLLECTION: Pairwise ranking dataset with diverse evaluation criteria.")
    st.markdown("- Weight merging technique for effective unified evaluator LMs.")
    
        
    st.code("""
    def logging_decorator(func): 
        # ...
        
    def timer_decorator(func):
        # ... (Code from above)
    """) 
    
if __name__ == "__main__":
    load_page()
